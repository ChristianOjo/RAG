{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ Mini RAG Chatbot - Interactive Demo\n",
    "\n",
    "This notebook demonstrates the complete RAG pipeline:\n",
    "1. Document loading and inspection\n",
    "2. Text chunking strategies\n",
    "3. Embedding visualization\n",
    "4. Retrieval testing\n",
    "5. End-to-end Q&A\n",
    "6. Failure case analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from ingest import DocumentIngestor\n",
    "from retrieval import Retriever\n",
    "from chatbot import RAGChatbot\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Document Loading üìÑ\n",
    "\n",
    "Let's load and inspect our research papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "\n",
    "# Load documents\n",
    "loader = DirectoryLoader(\n",
    "    \"../data\",\n",
    "    glob=\"**/*.pdf\",\n",
    "    loader_cls=PyPDFLoader\n",
    ")\n",
    "\n",
    "documents = loader.load()\n",
    "print(f\"‚úì Loaded {len(documents)} pages from {len(set([d.metadata['source'] for d in documents]))} documents\")\n",
    "\n",
    "# Inspect first document\n",
    "print(f\"\\nFirst page preview:\")\n",
    "print(f\"Source: {documents[0].metadata['source']}\")\n",
    "print(f\"Content length: {len(documents[0].page_content)} characters\")\n",
    "print(f\"\\nContent preview:\\n{documents[0].page_content[:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chunking Analysis üìä\n",
    "\n",
    "Analyze different chunking strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Test different chunk sizes\n",
    "chunk_sizes = [500, 1000, 1500, 2000]\n",
    "overlap = 200\n",
    "\n",
    "results = []\n",
    "\n",
    "for size in chunk_sizes:\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=size,\n",
    "        chunk_overlap=overlap\n",
    "    )\n",
    "    chunks = splitter.split_documents(documents)\n",
    "    \n",
    "    results.append({\n",
    "        'chunk_size': size,\n",
    "        'num_chunks': len(chunks),\n",
    "        'avg_chunk_len': np.mean([len(c.page_content) for c in chunks]),\n",
    "        'std_chunk_len': np.std([len(c.page_content) for c in chunks])\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(\"Chunking Strategy Comparison:\")\n",
    "print(df)\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.bar(df['chunk_size'], df['num_chunks'], color='steelblue')\n",
    "ax1.set_xlabel('Chunk Size')\n",
    "ax1.set_ylabel('Number of Chunks')\n",
    "ax1.set_title('Total Chunks by Size')\n",
    "\n",
    "ax2.errorbar(df['chunk_size'], df['avg_chunk_len'], yerr=df['std_chunk_len'], \n",
    "             marker='o', capsize=5, color='coral')\n",
    "ax2.set_xlabel('Target Chunk Size')\n",
    "ax2.set_ylabel('Actual Chunk Length (chars)')\n",
    "ax2.set_title('Chunk Length Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Recommended: chunk_size=1000 balances context and granularity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embedding Space Visualization üé®\n",
    "\n",
    "Visualize document embeddings in 2D using t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Create embeddings for sample chunks\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = splitter.split_documents(documents[:20])  # Sample first 20 pages\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embeddings = model.encode([c.page_content for c in chunks])\n",
    "\n",
    "print(f\"Generated embeddings: {embeddings.shape}\")\n",
    "\n",
    "# Reduce to 2D\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(chunks)-1))\n",
    "embeddings_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], \n",
    "                     c=range(len(chunks)), cmap='viridis', \n",
    "                     s=100, alpha=0.6, edgecolors='black')\n",
    "\n",
    "# Annotate some points\n",
    "for i in range(0, len(chunks), max(1, len(chunks)//10)):\n",
    "    plt.annotate(f'Chunk {i}', \n",
    "                (embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
    "                fontsize=8, alpha=0.7)\n",
    "\n",
    "plt.colorbar(scatter, label='Chunk Index')\n",
    "plt.title('Document Chunks in Embedding Space (t-SNE)', fontsize=14)\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Similar chunks cluster together in embedding space\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Retrieval Testing üîç\n",
    "\n",
    "Test the retrieval system with various queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize retriever (assumes vectorstore exists)\n",
    "try:\n",
    "    retriever = Retriever(vectorstore_dir=\"../vectorstore\", top_k=4)\n",
    "    print(\"‚úì Retriever loaded successfully\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Run '../src/ingest.py' first to create the vector store\")\n",
    "    print(f\"Error: {e}\")\n",
    "    retriever = None\n",
    "\n",
    "if retriever:\n",
    "    # Test queries\n",
    "    test_queries = [\n",
    "        \"What is the main contribution?\",\n",
    "        \"What methodology was used?\",\n",
    "        \"What were the results?\",\n",
    "        \"What are the limitations?\"\n",
    "    ]\n",
    "    \n",
    "    for query in test_queries:\n",
    "        print(f\"Query: '{query}'\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        results = retriever.search(query, k=2)\n",
    "        \n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"\\n[{i}] Score: {result['score']:.4f}\")\n",
    "            print(f\"Source: {result['source']}\")\n",
    "            print(f\"Content: {result['content'][:200]}...\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. End-to-End RAG Demo ü§ñ\n",
    "\n",
    "Complete question-answering pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize chatbot\n",
    "try:\n",
    "    chatbot = RAGChatbot(\n",
    "        vectorstore_dir=\"../vectorstore\",\n",
    "        model=\"llama3.2\",\n",
    "        top_k=4\n",
    "    )\n",
    "    print(\"‚úì Chatbot initialized\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Error initializing chatbot: {e}\")\n",
    "    print(\"Make sure Ollama is running and llama3.2 is installed\")\n",
    "    chatbot = None\n",
    "\n",
    "if chatbot:\n",
    "    # Demo questions\n",
    "    questions = [\n",
    "        \"What is the main research question addressed in these papers?\",\n",
    "        \"What datasets were used in the experiments?\",\n",
    "        \"What are the key findings?\"\n",
    "    ]\n",
    "    \n",
    "    for question in questions:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Q: {question}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        result = chatbot.answer(question)\n",
    "        \n",
    "        print(f\"A: {result['answer']}\\n\")\n",
    "        print(f\"üìö Sources:\")\n",
    "        for i, source in enumerate(result['sources'], 1):\n",
    "            print(f\"  [{i}] {source['source']} (score: {source['score']:.4f})\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Failure Case Analysis üêõ\n",
    "\n",
    "Demonstrate common failure modes and how we fixed them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FAILURE CASE 1: Hallucination (Answering Beyond Context)\\n\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Ask about something likely NOT in the documents\n",
    "tricky_question = \"What is the model's exact accuracy on ImageNet-1K?\"\n",
    "\n",
    "if chatbot:\n",
    "    result = chatbot.answer(tricky_question)\n",
    "    \n",
    "    print(f\"Q: {tricky_question}\")\n",
    "    print(f\"\\nA: {result['answer']}\")\n",
    "    print(f\"\\n‚úì Notice: The model admits when information is not available\")\n",
    "    print(f\"‚úì This is due to our improved prompt engineering\")\n",
    "else:\n",
    "    print(\"Chatbot not available for demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFAILURE CASE 2: Poor Retrieval (Irrelevant Chunks)\\n\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Compare retrieval quality with different top_k values\n",
    "test_query = \"What loss function was used?\"\n",
    "\n",
    "if retriever:\n",
    "    for k in [2, 4, 6]:\n",
    "        results = retriever.search(test_query, k=k)\n",
    "        avg_score = np.mean([r['score'] for r in results])\n",
    "        \n",
    "        print(f\"\\ntop_k={k}: Average relevance score = {avg_score:.4f}\")\n",
    "        print(f\"Best result: {results[0]['content'][:150]}...\")\n",
    "    \n",
    "    print(\"\\n‚úì Notice: top_k=4 provides good balance between coverage and precision\")\n",
    "else:\n",
    "    print(\"Retriever not available for demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Metrics üìä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "if retriever and chatbot:\n",
    "    # Measure retrieval time\n",
    "    test_query = \"What is deep learning?\"\n",
    "    \n",
    "    start = time.time()\n",
    "    _ = retriever.search(test_query)\n",
    "    retrieval_time = time.time() - start\n",
    "    \n",
    "    # Measure end-to-end time\n",
    "    start = time.time()\n",
    "    _ = chatbot.answer(test_query)\n",
    "    total_time = time.time() - start\n",
    "    \n",
    "    # Create performance summary\n",
    "    stats = retriever.get_stats()\n",
    "    \n",
    "    performance_data = {\n",
    "        'Metric': [\n",
    "            'Total Chunks',\n",
    "            'Chunk Size',\n",
    "            'Retrieval Time',\n",
    "            'Generation Time',\n",
    "            'Total Time',\n",
    "            'Embedding Model'\n",
    "        ],\n",
    "        'Value': [\n",
    "            stats['total_chunks'],\n",
    "            stats['chunk_size'],\n",
    "            f\"{retrieval_time:.3f}s\",\n",
    "            f\"{total_time - retrieval_time:.3f}s\",\n",
    "            f\"{total_time:.3f}s\",\n",
    "            'all-MiniLM-L6-v2'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    df_perf = pd.DataFrame(performance_data)\n",
    "    print(\"\\nPerformance Summary:\")\n",
    "    print(df_perf.to_string(index=False))\n",
    "    \n",
    "    # Visualize timing breakdown\n",
    "    timing_data = {\n",
    "        'Retrieval': retrieval_time,\n",
    "        'Generation': total_time - retrieval_time\n",
    "    }\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.bar(timing_data.keys(), timing_data.values(), color=['steelblue', 'coral'])\n",
    "    plt.ylabel('Time (seconds)')\n",
    "    plt.title('RAG Pipeline Timing Breakdown')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Components not available for performance testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Demo Complete!\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Chunking matters**: 1000 chars with 200 overlap works best\n",
    "2. **Embeddings cluster**: Similar content groups together\n",
    "3. **Retrieval is fast**: <1s for most queries\n",
    "4. **Prompt engineering**: Critical for reducing hallucinations\n",
    "5. **top_k tuning**: Balance between coverage and precision\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Try with your own research papers\n",
    "- Experiment with different chunk sizes\n",
    "- Test various Llama models (3.1, 3.2, etc.)\n",
    "- Add re-ranking for better retrieval\n",
    "- Implement hybrid search (keyword + semantic)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}