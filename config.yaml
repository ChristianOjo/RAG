# Mini RAG Chatbot Configuration
# Customize these settings for your use case

# Document Processing
ingestion:
  data_dir: "data"
  vectorstore_dir: "vectorstore"
  chunk_size: 1000
  chunk_overlap: 200
  embedding_model: "sentence-transformers/all-MiniLM-L6-v2"
  
# Retrieval Settings
retrieval:
  top_k: 4
  score_threshold: 0.7  # Optional: minimum similarity score
  
# LLM Settings
llm:
  model: "llama3.2"
  temperature: 0.1
  max_tokens: 512
  
# Prompt Templates
prompts:
  system: |
    You are a helpful research assistant. Answer questions based on the provided context from research papers.
    
  rag_template: |
    Context from research papers:
    {context}
    
    Question: {query}
    
    Instructions:
    - Answer based ONLY on the information in the context above
    - If the context doesn't contain enough information to answer, say so
    - Be concise but comprehensive
    - Cite specific details from the context when relevant
    
    Answer:

# Advanced Settings
advanced:
  # Use GPU if available (requires faiss-gpu)
  use_gpu: false
  
  # Batch size for embedding generation
  embedding_batch_size: 32
  
  # Cache embeddings to disk
  cache_embeddings: true
  
  # Enable query expansion
  query_expansion: false
  
  # Re-ranking (requires cross-encoder model)
  enable_reranking: false
  reranking_model: "cross-encoder/ms-marco-MiniLM-L-6-v2"
  
# Logging
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  file: "rag_chatbot.log"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"